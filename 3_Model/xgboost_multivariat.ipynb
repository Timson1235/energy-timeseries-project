{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import optuna \n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names to be compatible with XGBoost\"\"\"\n",
    "    df = df.copy()\n",
    "    # Remove brackets and clean special characters\n",
    "    df.columns = (df.columns\n",
    "                 .str.replace('[', '')\n",
    "                 .str.replace(']', '')\n",
    "                 .str.replace(' ', '_')\n",
    "                 .str.replace('(', '')\n",
    "                 .str.replace(')', '')\n",
    "                 .str.replace('ö', 'oe')  # Handle German special characters\n",
    "                 .str.replace('ä', 'ae')\n",
    "                 .str.replace('ü', 'ue'))\n",
    "    return df\n",
    "\n",
    "def create_lagged_features(df, target_col, lag_hours=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 24, 48, 72, 168, 336, 672, 1008, 2016, 8760]):\n",
    "    \"\"\"Create additional lagged features for time series data\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lags for a variety of time periods (e.g., 1h, 2h, 1 day, 1 week, etc.)\n",
    "    for lag in lag_hours:\n",
    "        df_copy[f'{target_col}_lag_{lag}h'] = df_copy[target_col].shift(lag)\n",
    "        \n",
    "    # Drop rows with NaN values created by lagging\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def preprocess_load_data(data, split_date='2023-09-30'):\n",
    "    \"\"\"Preprocess the load data including lagged features\"\"\"\n",
    "    # Create a copy of the data\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Store the original load column name\n",
    "    original_load_col = 'Gesamt (Netzlast) [MWh] Berechnete Auflösungen'\n",
    "    \n",
    "    # Convert Date column to datetime and sort\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Create lagged features before cleaning column names\n",
    "    df = create_lagged_features(df, target_col=original_load_col)\n",
    "    \n",
    "    # Clean column names\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Get the cleaned load column name\n",
    "    load_col = 'Gesamt_Netzlast_MWh_Berechnete_Aufloesungen'\n",
    "    \n",
    "    # Rename to 'load'\n",
    "    df = df.rename(columns={load_col: 'load'})\n",
    "    \n",
    "    # Select features for the model (using cleaned names)\n",
    "    base_features = [\n",
    "        'hour', 'dayofyear_cos', 'dayofweek', 'dayofweek_sin',\n",
    "        'is_workday', 'hour_cos', 'date_offset', 'dayofyear',\n",
    "        'Kernenergie_MWh_Berechnete_Aufloesungen',\n",
    "        'Steinkohle_MWh_Berechnete_Aufloesungen',\n",
    "        'Holiday_Not_a_Holiday', 'hour_sin',\n",
    "        'Wind_Onshore_MWh_Berechnete_Aufloesungen'\n",
    "    ]\n",
    "    \n",
    "    # Add lagged feature names\n",
    "    lag_features = [col for col in df.columns if 'lag' in col]\n",
    "    feature_columns = base_features + lag_features\n",
    "    \n",
    "    # Split data based on date\n",
    "    train_data = df[df['Date'] < split_date]\n",
    "    test_data = df[df['Date'] >= split_date]\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train = train_data[feature_columns]\n",
    "    y_train = train_data['load']\n",
    "    X_test = test_data[feature_columns]\n",
    "    y_test = test_data['load']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to pandas DataFrames\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns)\n",
    "    \n",
    "    print(f\"\\nTraining data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Testing data shape: {X_test_scaled.shape}\")\n",
    "    print(f\"Training period: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
    "    print(f\"Testing period: {test_data['Date'].min()} to {test_data['Date'].max()}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_cv(X_train, y_train, n_folds=5):\n",
    "    \"\"\"Evaluate multiple models using cross-validation on training data\"\"\"\n",
    "    \n",
    "    # Define base models with default parameters\n",
    "    models = {\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=2000,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=20,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=4000,\n",
    "            learning_rate=0.001,\n",
    "            max_depth=500,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name} using {n_folds}-fold cross-validation...\")\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=n_folds,\n",
    "            scoring='neg_mean_absolute_percentage_error',\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "        \n",
    "        # Convert negative MAPE scores to positive percentages\n",
    "        mape_scores = -scores * 100\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'cv_scores': mape_scores,\n",
    "            'mean_cv_mape': mape_scores.mean(),\n",
    "            'std_cv_mape': mape_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"Mean CV MAPE: {mape_scores.mean():.2f}% (±{mape_scores.std():.2f})\")\n",
    "        print(f\"Individual fold MAPEs: {[f'{score:.2f}%' for score in mape_scores]}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data shape: (34319, 34)\n",
      "Testing data shape: (9541, 34)\n",
      "Training period: 2019-10-31 18:00:00 to 2023-09-29 23:00:00\n",
      "Testing period: 2023-09-30 00:00:00 to 2024-10-30 23:00:00\n",
      "\n",
      "Evaluating XGBoost using 5-fold cross-validation...\n",
      "XGBoost Results:\n",
      "Mean CV MAPE: 0.86% (±0.12)\n",
      "Individual fold MAPEs: ['0.83%', '0.81%', '0.77%', '0.81%', '1.11%']\n",
      "\n",
      "Evaluating LightGBM using 5-fold cross-validation...\n",
      "LightGBM Results:\n",
      "Mean CV MAPE: 1.09% (±0.13)\n",
      "Individual fold MAPEs: ['1.05%', '1.05%', '1.02%', '1.00%', '1.34%']\n",
      "\n",
      "Evaluating RandomForest using 5-fold cross-validation...\n",
      "RandomForest Results:\n",
      "Mean CV MAPE: 0.89% (±0.10)\n",
      "Individual fold MAPEs: ['0.86%', '0.87%', '0.79%', '0.82%', '1.09%']\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Load the data\n",
    "df = pd.read_csv('../Data/selected_features.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test, feature_names = preprocess_load_data(df)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = evaluate_models_cv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names to be compatible with XGBoost\"\"\"\n",
    "    df = df.copy()\n",
    "    # Remove brackets and clean special characters\n",
    "    df.columns = (df.columns\n",
    "                 .str.replace('[', '')\n",
    "                 .str.replace(']', '')\n",
    "                 .str.replace(' ', '_')\n",
    "                 .str.replace('(', '')\n",
    "                 .str.replace(')', '')\n",
    "                 .str.replace('ö', 'oe')  # Handle German special characters\n",
    "                 .str.replace('ä', 'ae')\n",
    "                 .str.replace('ü', 'ue'))\n",
    "    return df\n",
    "\n",
    "def create_lagged_features(df, target_col, lag_hours=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 24, 48, 72, 168, 336, 672, 1008, 2016, 8760]):\n",
    "    \"\"\"Create additional lagged features for time series data\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create lags for a variety of time periods (e.g., 1h, 2h, 1 day, 1 week, etc.)\n",
    "    for lag in lag_hours:\n",
    "        df_copy[f'{target_col}_lag_{lag}h'] = df_copy[target_col].shift(lag)\n",
    "        \n",
    "    # Drop rows with NaN values created by lagging\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def preprocess_load_data(data, split_date='2023-09-30'):\n",
    "    \"\"\"Preprocess the load data including lagged features\"\"\"\n",
    "    # Create a copy of the data\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Store the original load column name\n",
    "    original_load_col = 'Gesamt (Netzlast) [MWh] Berechnete Auflösungen'\n",
    "    \n",
    "    # Convert Date column to datetime and sort\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Create lagged features before cleaning column names\n",
    "    df = create_lagged_features(df, target_col=original_load_col)\n",
    "    \n",
    "    # Clean column names\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Get the cleaned load column name\n",
    "    load_col = 'Gesamt_Netzlast_MWh_Berechnete_Aufloesungen'\n",
    "    \n",
    "    # Rename to 'load'\n",
    "    df = df.rename(columns={load_col: 'load'})\n",
    "    \n",
    "    # Select features for the model (using cleaned names)\n",
    "    base_features = [\n",
    "        'hour', 'dayofyear_cos', 'dayofweek', 'dayofweek_sin',\n",
    "        'is_workday', 'hour_cos', 'date_offset', 'dayofyear',\n",
    "        'Kernenergie_MWh_Berechnete_Aufloesungen',\n",
    "        'Steinkohle_MWh_Berechnete_Aufloesungen',\n",
    "        'Holiday_Not_a_Holiday', 'hour_sin',\n",
    "        'Wind_Onshore_MWh_Berechnete_Aufloesungen'\n",
    "    ]\n",
    "    \n",
    "    # Add lagged feature names\n",
    "    lag_features = [col for col in df.columns if 'lag' in col]\n",
    "    feature_columns = base_features + lag_features\n",
    "    \n",
    "    # Split data based on date\n",
    "    train_data = df[df['Date'] < split_date]\n",
    "    test_data = df[df['Date'] >= split_date]\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train = train_data[feature_columns]\n",
    "    y_train = train_data['load']\n",
    "    X_test = test_data[feature_columns]\n",
    "    y_test = test_data['load']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to pandas DataFrames\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns)\n",
    "    \n",
    "    print(f\"\\nTraining data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Testing data shape: {X_test_scaled.shape}\")\n",
    "    print(f\"Training period: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
    "    print(f\"Testing period: {test_data['Date'].min()} to {test_data['Date'].max()}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, feature_columns\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate multiple models\"\"\"\n",
    "    models = {\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.001,\n",
    "            max_depth=30,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.001,\n",
    "            max_depth=30,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate MAPE\n",
    "        train_mape = mean_absolute_percentage_error(y_train, train_pred) * 100\n",
    "        test_mape = mean_absolute_percentage_error(y_test, test_pred) * 100\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_mape': train_mape,\n",
    "            'test_mape': test_mape,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"Train MAPE: {train_mape:.2f}%\")\n",
    "        print(f\"Test MAPE: {test_mape:.2f}%\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train and evaluate models\n",
    "results = train_and_evaluate_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save models\n",
    "joblib.dump(results['XGBoost']['model'], 'xgboost_model.pkl')\n",
    "joblib.dump(results['LightGBM']['model'], 'lightgbm_model.pkl')\n",
    "joblib.dump(results['RandomForest']['model'], 'randomforest_model.pkl')\n",
    "\n",
    "# # To load the models later:\n",
    "# xgboost_model = joblib.load('xgboost_model.pkl')\n",
    "# lightgbm_model = joblib.load('lightgbm_model.pkl')\n",
    "# randomforest_model = joblib.load('randomforest_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m xgboost_pred \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXGBoost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      3\u001b[0m lightgbm_pred \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m randomforest_pred \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "xgboost_pred = results['XGBoost']['model'].predict(X_test)\n",
    "lightgbm_pred = results['LightGBM']['model'].predict(X_test)\n",
    "randomforest_pred = results['RandomForest']['model'].predict(X_test)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=True)\n",
    "\n",
    "# XGBoost plot\n",
    "axes[0].plot(y_test.index, y_test, label='Actual Load', color='blue')\n",
    "axes[0].plot(y_test.index, xgboost_pred, label='XGBoost Forecast', color='green', linestyle='--')\n",
    "axes[0].set_title('XGBoost: Actual vs Forecasted Load')\n",
    "axes[0].set_ylabel('Load [MWh]')\n",
    "axes[0].legend()\n",
    "\n",
    "# LightGBM plot\n",
    "axes[1].plot(y_test.index, y_test, label='Actual Load', color='blue')\n",
    "axes[1].plot(y_test.index, lightgbm_pred, label='LightGBM Forecast', color='red', linestyle='--')\n",
    "axes[1].set_title('LightGBM: Actual vs Forecasted Load')\n",
    "axes[1].set_ylabel('Load [MWh]')\n",
    "axes[1].legend()\n",
    "\n",
    "# RandomForest plot\n",
    "axes[2].plot(y_test.index, y_test, label='Actual Load', color='blue')\n",
    "axes[2].plot(y_test.index, randomforest_pred, label='RandomForest Forecast', color='orange', linestyle='--')\n",
    "axes[2].set_title('RandomForest: Actual vs Forecasted Load')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Load [MWh]')\n",
    "axes[2].legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'2023-10-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '2023-10-01'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m week_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-10-07\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Filter the data for the specified week\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m y_test_week \u001b[38;5;241m=\u001b[39m \u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mweek_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mweek_end\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m xgboost_pred_week \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(xgboost_pred, index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mloc[week_start:week_end]\n\u001b[0;32m      8\u001b[0m lightgbm_pred_week \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(lightgbm_pred, index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mloc[week_start:week_end]\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1411\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1443\u001b[0m, in \u001b[0;36m_LocIndexer._get_slice_axis\u001b[1;34m(self, slice_obj, axis)\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1442\u001b[0m labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 1443\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6662\u001b[0m, in \u001b[0;36mIndex.slice_indexer\u001b[1;34m(self, start, end, step)\u001b[0m\n\u001b[0;32m   6618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_indexer\u001b[39m(\n\u001b[0;32m   6619\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   6620\u001b[0m     start: Hashable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6621\u001b[0m     end: Hashable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6622\u001b[0m     step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6623\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mslice\u001b[39m:\n\u001b[0;32m   6624\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6625\u001b[0m \u001b[38;5;124;03m    Compute the slice indexer for input labels and step.\u001b[39;00m\n\u001b[0;32m   6626\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6660\u001b[0m \u001b[38;5;124;03m    slice(1, 3, None)\u001b[39;00m\n\u001b[0;32m   6661\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6662\u001b[0m     start_slice, end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6664\u001b[0m     \u001b[38;5;66;03m# return a slice\u001b[39;00m\n\u001b[0;32m   6665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(start_slice):\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6879\u001b[0m, in \u001b[0;36mIndex.slice_locs\u001b[1;34m(self, start, end, step)\u001b[0m\n\u001b[0;32m   6877\u001b[0m start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 6879\u001b[0m     start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_slice_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_slice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6881\u001b[0m     start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6804\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[1;34m(self, label, side)\u001b[0m\n\u001b[0;32m   6801\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_searchsorted_monotonic(label, side)\n\u001b[0;32m   6802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   6803\u001b[0m         \u001b[38;5;66;03m# raise the original KeyError\u001b[39;00m\n\u001b[1;32m-> 6804\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m   6806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   6807\u001b[0m     \u001b[38;5;66;03m# get_loc may return a boolean array, which\u001b[39;00m\n\u001b[0;32m   6808\u001b[0m     \u001b[38;5;66;03m# is OK as long as they are representable by a slice.\u001b[39;00m\n\u001b[0;32m   6809\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m is_bool_dtype(slc\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6798\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[1;34m(self, label, side)\u001b[0m\n\u001b[0;32m   6796\u001b[0m \u001b[38;5;66;03m# we need to look up the label\u001b[39;00m\n\u001b[0;32m   6797\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6798\u001b[0m     slc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   6800\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TimPr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '2023-10-01'"
     ]
    }
   ],
   "source": [
    "# Define the range for the week to zoom in on (adjust as needed)\n",
    "week_start = '2023-10-01'\n",
    "week_end = '2023-10-07'\n",
    "\n",
    "# Filter the data for the specified week\n",
    "y_test_week = y_test.loc[week_start:week_end]\n",
    "xgboost_pred_week = pd.Series(xgboost_pred, index=y_test.index).loc[week_start:week_end]\n",
    "lightgbm_pred_week = pd.Series(lightgbm_pred, index=y_test.index).loc[week_start:week_end]\n",
    "randomforest_pred_week = pd.Series(randomforest_pred, index=y_test.index).loc[week_start:week_end]\n",
    "\n",
    "# Create subplots for the zoomed-in week\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=True)\n",
    "\n",
    "# XGBoost plot\n",
    "axes[0].plot(y_test_week.index, y_test_week, label='Actual Load', color='blue')\n",
    "axes[0].plot(y_test_week.index, xgboost_pred_week, label='XGBoost Forecast', color='green', linestyle='--')\n",
    "axes[0].set_title('XGBoost: Actual vs Forecasted Load (Zoomed-In Week)')\n",
    "axes[0].set_ylabel('Load [MWh]')\n",
    "axes[0].legend()\n",
    "\n",
    "# LightGBM plot\n",
    "axes[1].plot(y_test_week.index, y_test_week, label='Actual Load', color='blue')\n",
    "axes[1].plot(y_test_week.index, lightgbm_pred_week, label='LightGBM Forecast', color='red', linestyle='--')\n",
    "axes[1].set_title('LightGBM: Actual vs Forecasted Load (Zoomed-In Week)')\n",
    "axes[1].set_ylabel('Load [MWh]')\n",
    "axes[1].legend()\n",
    "\n",
    "# RandomForest plot\n",
    "axes[2].plot(y_test_week.index, y_test_week, label='Actual Load', color='blue')\n",
    "axes[2].plot(y_test_week.index, randomforest_pred_week, label='RandomForest Forecast', color='orange', linestyle='--')\n",
    "axes[2].set_title('RandomForest: Actual vs Forecasted Load (Zoomed-In Week)')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Load [MWh]')\n",
    "axes[2].legend()\n",
    "\n",
    "# Display the zoomed-in plots\n",
    "plt.xticks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CV Hyperparametersearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimize_and_evaluate_models_cv(X_train, y_train, n_folds=5, n_trials=10):\n",
    "    \"\"\"Quick test for optimizing and evaluating models using cross-validation\"\"\"\n",
    "    \n",
    "    def objective_xgb(trial, X, y):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators', [500, 1000, 2000, 4000]),\n",
    "            'max_depth': trial.suggest_int('max_depth', 10, 30),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        scores = cross_val_score(\n",
    "            model, X, y,\n",
    "            cv=n_folds,\n",
    "            scoring='neg_mean_absolute_percentage_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return -scores.mean() * 100\n",
    "\n",
    "    def objective_lgb(trial, X, y):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators', [500, 1000, 2000, 4000]),\n",
    "            'max_depth': trial.suggest_int('max_depth', 10, 30),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        scores = cross_val_score(\n",
    "            model, X, y,\n",
    "            cv=n_folds,\n",
    "            scoring='neg_mean_absolute_percentage_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return -scores.mean() * 100\n",
    "\n",
    "    results = {}\n",
    "    objectives = {\n",
    "        'XGBoost': objective_xgb,\n",
    "        'LightGBM': objective_lgb,\n",
    "    }\n",
    "\n",
    "    for name, objective in objectives.items():\n",
    "        print(f\"\\nTesting {name} optimization...\")\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, y_train),\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"\\n{name} Best parameters (test): {best_params}\")\n",
    "        \n",
    "        # Test evaluation\n",
    "        print(f\"\\nEvaluating best {name} model (test)...\")\n",
    "        if name == 'XGBoost':\n",
    "            best_model = xgb.XGBRegressor(**best_params)\n",
    "        elif name == 'LightGBM':\n",
    "            best_model = lgb.LGBMRegressor(**best_params)\n",
    "        \n",
    "        scores = cross_val_score(\n",
    "            best_model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=n_folds,\n",
    "            scoring='neg_mean_absolute_percentage_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        mape_scores = -scores * 100\n",
    "        print(f\"{name} Test Results:\")\n",
    "        print(f\"Mean CV MAPE: {mape_scores.mean():.2f}%\")\n",
    "        print(f\"Std CV MAPE: {mape_scores.std():.2f}%\")\n",
    "        results[name] = mape_scores.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage:\n",
    "# test_results = test_optimize_and_evaluate_models_cv(X_train, y_train, n_folds=3, n_trials=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 11:38:52,641] A new study created in memory with name: no-name-ec643319-2f0b-4e01-8019-5ecac19b95e1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing XGBoost optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TimPr\\AppData\\Local\\Temp\\ipykernel_14884\\2037188567.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n"
     ]
    }
   ],
   "source": [
    "test_results = test_optimize_and_evaluate_models_cv(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
